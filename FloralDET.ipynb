{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FloralDET.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBMcobPHdD8O"
      },
      "source": [
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "  \n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NOG3l_MsBO1A",
        "outputId": "12062a29-a701-43e3-ef48-92394d3a0327"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxL2mjVVGIrV"
      },
      "source": [
        "\n",
        "\n",
        "base_dir = \"/content/drive/My Drive/Colab Notebooks/jpg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZt4V_UJJs_B",
        "outputId": "a3fcff37-afd3-4af4-cdd3-904c758274ab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aCLb_yV5JfF3"
      },
      "source": [
        "IMAGE_SIZE = 224\n",
        "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "\n",
        "BATCH_SIZE1 = 5\n",
        "BATCH_SIZE2=10\n",
        "BATCH_SIZE3=20\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255, \n",
        "    validation_split=0.1,samplewise_std_normalization=True)\n",
        "\n",
        "\n",
        "train_generator1 = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE1, \n",
        "    subset='training')\n",
        "\n",
        "val_generator1 = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE1, \n",
        "    subset='validation')\n",
        "\n",
        "train_generator2 = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE2, \n",
        "    subset='training')\n",
        "\n",
        "val_generator2 = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE2, \n",
        "    subset='validation')\n",
        "\n",
        "train_generator3 = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE3, \n",
        "    subset='training')\n",
        "\n",
        "val_generator3 = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE3, \n",
        "    subset='validation')\n",
        "\n",
        "\n",
        "\n",
        "base_model1=tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                              include_top=False, \n",
        "                                              weights='imagenet')\n",
        "\n",
        "base_model2=tf.keras.applications.MobileNet(input_shape=IMG_SHAPE,\n",
        "                                              include_top=False, \n",
        "                                              weights='imagenet')\n",
        "\n",
        "base_model3=tf.keras.applications.MobileNetV3(input_shape=IMG_SHAPE,\n",
        "                                              include_top=False, \n",
        "                                              weights='imagenet')\n",
        "optimizer1=tf.keras.optimizers.Adagrad()\n",
        "optimizer2=tf.keras.optimizers.Adam(1e-5)\n",
        "optimizer3=tf.keras.optimizers.RMSprop()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tx1L7fxxWA_G"
      },
      "source": [
        "for image_batch, label_batch in train_generator1:\n",
        "  break\n",
        "image_batch.shape, label_batch.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-QFZIhWs4dsq"
      },
      "source": [
        "print (train_generator1.class_indices)\n",
        "\n",
        "labels = '\\n'.join(sorted(train_generator1.class_indices.keys()))\n",
        "\n",
        "with open('labels.txt', 'w') as f:\n",
        "  f.write(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "duxD_UDSOmng"
      },
      "source": [
        "!cat labels.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vA0S1_R_wVVe"
      },
      "source": [
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\", \n",
        "                                                 input_shape=(IMAGE_SIZE, \n",
        "                                                              IMAGE_SIZE,\n",
        "                                                              3)),\n",
        "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "    \n",
        "  ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eApvroIyn1K0"
      },
      "source": [
        "\n",
        "def models(base_model,opt,train_generator,val_generator):\n",
        "  base_model.trainable = False\n",
        "  model = tf.keras.Sequential([\n",
        "  data_augmentation,\n",
        "  base_model,\n",
        "    \n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    \n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    \n",
        "  tf.keras.layers.Dense(17, activation='softmax',kernel_regularizer=\"l2\")\n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adagrad(), \n",
        "                loss='categorical_crossentropy', \n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  epochs = 20\n",
        "\n",
        "  model.fit(train_generator, \n",
        "                      steps_per_epoch=len(train_generator), \n",
        "                      epochs=epochs, \n",
        "                      validation_data=val_generator, \n",
        "                      validation_steps=len(val_generator))\n",
        "\n",
        "  \n",
        "\n",
        "  base_model.trainable = True\n",
        "\n",
        "\n",
        "  # Fine tune from this layer onwards\n",
        "  fine_tune_at = 100\n",
        "\n",
        "  # Freeze all the layers before the `fine_tune_at` layer\n",
        "  for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable =  False\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer = opt,\n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  history_fine = model.fit(train_generator, \n",
        "                          steps_per_epoch=len(train_generator), \n",
        "                          epochs=10, \n",
        "                          validation_data=val_generator, \n",
        "                          validation_steps=len(val_generator))\n",
        "\n",
        "\n",
        "  acc = history_fine.history['accuracy']\n",
        "  val_acc = history_fine.history['val_accuracy']\n",
        "\n",
        "  loss = history_fine.history['loss']\n",
        "  val_loss = history_fine.history['val_loss']\n",
        "\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  plt.subplot(2, 1, 1)\n",
        "  plt.plot(acc, label='Training Accuracy')\n",
        "  plt.plot(val_acc, label='Validation Accuracy')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.ylim([min(plt.ylim()),1])\n",
        "  plt.title('Training and Validation Accuracy')\n",
        "\n",
        "  plt.subplot(2, 1, 2)\n",
        "  plt.plot(loss, label='Training Loss')\n",
        "  plt.plot(val_loss, label='Validation Loss')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.ylabel('Cross Entropy')\n",
        "  plt.ylim([0,4.0])\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.show()\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ji-qz_PK30-P"
      },
      "source": [
        "hparam=[(base_model1,optimizer1,train_generator1,val_generator1),\n",
        "        (base_model1,optimizer1,train_generator2,val_generator2),\n",
        "        (base_model1,optimizer1,train_generator3,val_generator3),\n",
        "        (base_model1,optimizer2,train_generator1,val_generator1),\n",
        "        (base_model1,optimizer2,train_generator2,val_generator2),\n",
        "        (base_model1,optimizer2,train_generator3,val_generator3),\n",
        "        (base_model1,optimizer3,train_generator1,val_generator1),\n",
        "        (base_model1,optimizer3,train_generator2,val_generator2),\n",
        "        (base_model1,optimizer3,train_generator3,val_generator3),\n",
        "\n",
        "        (base_model2,optimizer1,train_generator1,val_generator1),\n",
        "        (base_model2,optimizer1,train_generator2,val_generator2),\n",
        "        (base_model2,optimizer1,train_generator3,val_generator3),\n",
        "        (base_model2,optimizer2,train_generator1,val_generator1),\n",
        "        (base_model2,optimizer2,train_generator2,val_generator2),\n",
        "        (base_model2,optimizer2,train_generator3,val_generator3),\n",
        "        (base_model2,optimizer3,train_generator1,val_generator1),\n",
        "        (base_model2,optimizer3,train_generator2,val_generator2),\n",
        "        (base_model2,optimizer3,train_generator3,val_generator3),\n",
        "\n",
        "        (base_model3,optimizer1,train_generator1,val_generator1),\n",
        "        (base_model3,optimizer1,train_generator2,val_generator2),\n",
        "        (base_model3,optimizer1,train_generator3,val_generator3),\n",
        "        (base_model3,optimizer2,train_generator1,val_generator1),\n",
        "        (base_model3,optimizer2,train_generator2,val_generator2),\n",
        "        (base_model3,optimizer2,train_generator3,val_generator3),\n",
        "        (base_model3,optimizer3,train_generator1,val_generator1),\n",
        "        (base_model3,optimizer3,train_generator2,val_generator2),\n",
        "        (base_model3,optimizer3,train_generator3,val_generator3)\n",
        "\n",
        "        ]\n",
        "\n",
        "Models=[]\n",
        "for i in hparam:\n",
        "  print(\"Model: {}\".format(hparam.index(i)+1))\n",
        "  Models.append(models(i[0],i[1],i[2],i[3]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QVHQxQWSnNXa"
      },
      "source": [
        "Models[13].save(\"model.h5\")\n",
        " \n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}